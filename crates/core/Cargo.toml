[package]
name = "localgpt-core"
version.workspace = true
edition.workspace = true
license.workspace = true
repository.workspace = true
description = "Core library for LocalGPT — agent, memory, config, security"

[features]
default = ["embeddings-local", "claude-cli"]
# Local embeddings via fastembed (ONNX). Desktop default.
embeddings-local = ["fastembed"]
# Claude CLI provider (requires subprocess execution — not available on mobile)
claude-cli = []
# GGUF embedding model support via llama.cpp (requires C++ compiler)
embeddings-gguf = ["llama-cpp-2"]
# OpenAI API embeddings (no native deps, always works on mobile)
embeddings-openai = []
# Disable all embeddings — FTS5 keyword search only
embeddings-none = []
# Legacy alias
gguf = ["embeddings-gguf"]

[dependencies]
tokio = { workspace = true }
reqwest = { workspace = true }
rusqlite = { workspace = true }
sqlite-vec = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
serde_yaml = { workspace = true }
toml = { workspace = true }
tracing = { workspace = true }
chrono = { workspace = true }
uuid = { workspace = true }
async-trait = { workspace = true }
futures = { workspace = true }
anyhow = { workspace = true }
thiserror = { workspace = true }
shellexpand = { workspace = true }
rand = { workspace = true }

# Local embeddings (default - no API key needed)
fastembed = { version = "5.9", optional = true }
# GGUF embeddings via llama.cpp (optional)
llama-cpp-2 = { version = "0.1", optional = true }

# Config
json5 = "1.3"

# File watching
notify = "8.2"

# Token counting
tiktoken-rs = "0.9"

# Path resolution
directories = "6.0"
etcetera = "0.8"
libc = "0.2"

# Utilities
tokio-stream = "0.1"
async-stream = "0.3"
glob = "0.3"
base64 = "0.22"
regex = "1"
once_cell = "1"
fs2 = "0.4"

# Security (HMAC signing, hashing)
sha2 = "0.10"
hmac = "0.12"

[target.'cfg(target_os = "linux")'.build-dependencies]
cc = "1"

[dev-dependencies]
tempfile = "3.25"
mockall = "0.14"
