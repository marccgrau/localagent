# LocalGPT Configuration
# Copy to ~/.localgpt/config.toml

[agent]
# Default model to use for chat
#
# Format: "provider/model-id" (OpenClaw-compatible)
#
# Anthropic API (recommended, requires ANTHROPIC_API_KEY):
#   - "anthropic/claude-opus-4-5" (recommended)
#   - "anthropic/claude-sonnet-4-5"
#
# Short aliases (auto-resolved to latest 4.5 models):
#   - "opus"   → anthropic/claude-opus-4-5
#   - "sonnet" → anthropic/claude-sonnet-4-5
#   - "gpt"    → openai/gpt-4o
#
# OpenAI API (requires OPENAI_API_KEY):
#   - "openai/gpt-4o", "openai/gpt-4o-mini", "openai/gpt-4-turbo"
#
# xAI API (requires XAI_API_KEY):
#   - "xai/grok-3-mini", "xai/grok-3"
#
# GLM / Z.AI API (requires GLM API key):
#   - "glm/glm-4.7" (or use alias "glm")
#
# Claude CLI (local, no API key needed):
#   - "claude-cli/opus", "claude-cli/sonnet", "claude-cli/haiku"
#
# Ollama (local):
#   - "ollama/llama3", "ollama/mistral", etc.
#
default_model = "claude-cli/opus"

# Context window size (in tokens)
context_window = 128000

# Reserve tokens for response
reserve_tokens = 8000

# Anthropic configuration (REQUIRED for default model)
# Get your API key at: https://console.anthropic.com/
[providers.anthropic]
api_key = "${ANTHROPIC_API_KEY}"  # Set: export ANTHROPIC_API_KEY="sk-ant-..."
base_url = "https://api.anthropic.com"

# OpenAI configuration (optional)
# [providers.openai]
# api_key = "${OPENAI_API_KEY}"
# base_url = "https://api.openai.com/v1"
# For local OpenAI-compatible servers (LM Studio, llamafile, vLLM):
# api_key = "not-needed"
# base_url = "http://127.0.0.1:8080/v1" # LM Studio default is http://127.0.0.1:1234/v1

# xAI configuration (optional, for xai/* models)
# [providers.xai]
# api_key = "${XAI_API_KEY}"
# base_url = "https://api.x.ai/v1"

# Ollama configuration (for local models)
# [providers.ollama]
# endpoint = "http://localhost:11434"
# model = "llama3"

# Claude CLI configuration (uses local claude CLI command)
# Requires claude CLI to be installed: https://github.com/anthropics/claude-code
# [providers.claude_cli]
# command = "claude"
# model = "opus"  # opus, sonnet, or haiku

# GLM / Z.AI configuration (optional)
# Get your API key at: https://z.ai/manage-apikey/apikey-list
# [providers.glm]
# api_key = "${GLM_API_KEY}"
# base_url = "https://api.z.ai/api/coding/paas/v4"

# Anthropic OAuth configuration (for Claude Pro/Max subscription plans)
# Use this instead of api_key-based authentication to leverage your subscription quota
# To obtain OAuth tokens:
#   1. Use PKCE flow with client_id: 9d1c250a-e61b-44d9-88ed-5944d1962f5e
#   2. Authorize at: https://claude.ai/oauth/authorize
#   3. Exchange code at: https://console.anthropic.com/v1/oauth/token
# See: https://github.com/querymt/anthropic-auth for implementation examples
# [providers.anthropic_oauth]
# access_token = "${ANTHROPIC_OAUTH_TOKEN}"
# refresh_token = "${ANTHROPIC_OAUTH_REFRESH_TOKEN}"  # Optional, for token renewal
# base_url = "https://api.anthropic.com"

# Gemini OAuth configuration (for Google AI subscription plans)
# Use this for Google Gemini subscription authentication with OAuth 2.0
# To obtain OAuth tokens:
#   1. Enable Gemini API in Google Cloud Console
#   2. Configure OAuth consent screen
#   3. Create OAuth 2.0 credentials (Desktop app or Web app)
#   4. Use authorization code flow to get access_token
# See: https://ai.google.dev/gemini-api/docs/oauth
# [providers.gemini_oauth]
# access_token = "${GEMINI_OAUTH_TOKEN}"
# refresh_token = "${GEMINI_OAUTH_REFRESH_TOKEN}"  # Optional, for token renewal
# base_url = "https://generativelanguage.googleapis.com"
# project_id = "${GOOGLE_CLOUD_PROJECT}"  # Optional, for enterprise/subscription plans

[heartbeat]
# Enable automatic heartbeat
enabled = true

# How often to check HEARTBEAT.md
interval = "30m"

# Maximum wall-clock time for a single heartbeat run (optional).
# If the heartbeat LLM turn exceeds this deadline it is cancelled and
# a TimedOut event is recorded so the next interval can run on schedule.
# Defaults to half the interval (e.g., "15m" when interval = "30m").
# timeout = "15m"

# Only run during these hours (optional)
# [heartbeat.active_hours]
# start = "09:00"
# end = "22:00"

[memory]
# Where to store memory files
workspace = "~/.localgpt/workspace"

# Embedding provider for semantic search: "local" (default), "gguf", "openai", or "none"
# - "local": Uses FastEmbed/ONNX (all-MiniLM-L6-v2), no API key needed
# - "gguf": Uses llama.cpp for GGUF models (requires --features gguf build)
# - "openai": Uses OpenAI embeddings (requires providers.openai config)
# - "none": FTS-only search, no vector embeddings
embedding_provider = "local"

# Embedding model for local provider (FastEmbed):
#
# English:
#   - all-MiniLM-L6-v2   (default, ~80MB, fastest)
#   - bge-base-en-v1.5   (~430MB, higher quality)
#
# Chinese:
#   - bge-small-zh-v1.5  (~95MB, Chinese-specific)
#
# Multilingual (Chinese, Japanese, Korean, 100+ languages):
#   - multilingual-e5-small  (~470MB, compact)
#   - multilingual-e5-base   (~1.1GB, recommended for Chinese)
#   - bge-m3                 (~2.2GB, best quality)
#
# For OpenAI provider: text-embedding-3-small, text-embedding-3-large
#
# For GGUF provider (requires --features gguf):
#   - embeddinggemma-300M-Q8_0.gguf  (~320MB, 1024 dims, multilingual)
#   - nomic-embed-text-v1.5.Q8_0.gguf (~270MB, 768 dims)
#   Note: Download models from HuggingFace and place in embedding_cache_dir
embedding_model = "all-MiniLM-L6-v2"

# Cache directory for local embedding models (ONNX format)
# Models are downloaded from HuggingFace on first use
# Default: ~/.cache/localgpt/models
# Can also be set via FASTEMBED_CACHE_DIR environment variable
# embedding_cache_dir = "~/.cache/localgpt/models"

# Chunk size for indexing (tokens)
chunk_size = 400

# Overlap between chunks (tokens)
chunk_overlap = 80

[server]
# Enable HTTP server
enabled = true

# Port to listen on
port = 31327

# Bind address (127.0.0.1 for localhost only)
bind = "127.0.0.1"

# Web search (optional)
# [tools.web_search]
# provider = "searxng"            # searxng | brave | tavily | perplexity | none
# cache_enabled = true
# cache_ttl = 900                 # seconds (default: 15 min)
# max_results = 5                 # 1-10
# prefer_native = true            # use provider-native search if available
#
# [tools.web_search.searxng]
# base_url = "http://localhost:8080"
# categories = "general"
# language = "en"
# time_range = ""                 # day | week | month | year | ""
#
# [tools.web_search.brave]
# api_key = "${BRAVE_API_KEY}"
# country = ""
# freshness = ""                  # pd | pw | pm | ""
#
# [tools.web_search.tavily]
# api_key = "${TAVILY_API_KEY}"
# search_depth = "basic"          # basic | advanced
# include_answer = true
#
# [tools.web_search.perplexity]
# api_key = "${PERPLEXITY_API_KEY}"
# model = "sonar"                 # sonar | sonar-pro | sonar-reasoning-pro

# Telegram bot (optional)
# Create a bot via @BotFather on Telegram to get an API token
# [telegram]
# enabled = true
# api_token = "${TELEGRAM_BOT_TOKEN}"

# Sandbox configuration for shell command isolation
# Every tool-executed shell command runs in a kernel-enforced sandbox
[sandbox]
# Enable sandbox (default: true)
enabled = true

# Sandbox enforcement level (default: "auto")
# Options:
#   - "auto"     — Use highest available level for your platform
#   - "full"     — Landlock V4+ + seccomp + userns (Linux only)
#   - "standard" — Landlock V1+ + seccomp (Linux) or Seatbelt (macOS)
#   - "minimal"  — seccomp network blocking only (Linux)
#   - "none"     — rlimits only, no filesystem or network isolation
level = "auto"

# Command timeout (default: 120 seconds)
timeout_secs = 120

# Maximum stdout+stderr bytes (default: 1MB = 1048576)
max_output_bytes = 1048576

# Maximum file size via RLIMIT_FSIZE (default: 50MB = 52428800)
max_file_size_bytes = 52428800

# Maximum child processes via RLIMIT_NPROC (default: 64)
max_processes = 64

# Network policy (default: "deny")
# Options: "deny" (block all network) or "proxy" (future: allow via socket)
[sandbox.network]
policy = "deny"

# Additional filesystem access beyond workspace and system directories
# [sandbox.allow_paths]
# read = ["/opt/data", "/mnt/shared"]
# write = ["/tmp/scratch"]

[security]
# LocalGPT injects a security block at the end of every LLM context window.
# The block has two independent layers:
#
#   1. User policy (LocalGPT.md) — your custom instructions, cryptographically
#      signed and verified. Injected as "## Workspace Security Policy" header
#      followed by your content. Requires `localgpt md sign` after editing.
#
#   2. Hardcoded suffix — a compiled-in security reminder that tells the model
#      to treat tool outputs and retrieved content as data, not instructions.
#      This is the last thing the model sees before generating a response.
#
# Both are concatenated into the last user message on every API call.
# They are NOT saved to session logs or included in compaction.
#
# You can disable each layer independently:

# Abort on tamper or suspicious content in LocalGPT.md (default: false)
# When true, TamperDetected and SuspiciousContent are fatal errors that
# prevent the agent from starting. When false, the agent warns and falls
# back to hardcoded suffix only.
# strict_policy = false

# Skip loading the LocalGPT.md workspace security policy (default: false)
# The hardcoded suffix still applies unless also disabled.
# disable_policy = false

# Skip the hardcoded security suffix (default: false)
# The user policy still applies unless also disabled.
# WARNING: Disabling both disable_policy and disable_suffix removes all
# end-of-context security reinforcement. The system prompt safety section
# still exists but may lose effectiveness in long sessions due to the
# "lost in the middle" attention decay.
# disable_suffix = false

[logging]
# Log level: trace, debug, info, warn, error
level = "info"

# Log file path
file = "~/.localgpt/logs/agent.log"
